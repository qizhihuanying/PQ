# Repository Guidelines

## Project Structure & Module Organization
`main.py` orchestrates MIRACL training runs, delegating batching to `trainer.MultiModelTrainer` and quantization logic in `product_quantization.PQHead`. Embedding adapters live in `_models/` (OpenAI, Anthropic, Cohere, HuggingFace, Voyage) with shared helpers in `_models/utils.py`. Raw MIRACL assets stay under `datasets/miracl/`, while processed pickles now live in `datasets/processed/<lang>/<split>/processed_data.pkl` generated by `make_dataset.py`. Experiments, metrics, and plots are collected in `analyze/`, while shared utilities (metrics, tables, string cleaning) reside in `utils/`. Automation scripts in `scripts/` plus the expected `logs/` folders illustrate the target directory layout for saved checkpoints and run logs.

## Build, Test, and Development Commands
- Create an isolated environment: `python3 -m venv .venv && source .venv/bin/activate` (PowerShell: `.\\.venv\\Scripts\\Activate.ps1`).
- Install dependencies: `pip install -r requirements.txt`.
- Prepare data: `python make_dataset.py --langs en ja` (append `--force_download` to refresh MIRACL assets).
- Start a single run: `python main.py --langs en --use_pq --epochs 3 --init_pq_path project/models/pq_head_initialized/...`.
- Sweep settings in bulk with `bash scripts/run_experiments.sh`, adjusting GPU IDs and output targets first.

## Coding Style & Naming Conventions
Stick to Python 3.10 style: 4-space indentation, `snake_case` for functions and variables, and `UpperCamelCase` for classes such as `PQHead`. Extend CLIs by mirroring existing flag names, update bilingual logging strings when necessary, and rely on `logging` instead of ad-hoc prints. Add type hints and small helpers to keep modules readable.

## Testing Guidelines
There is no pytest suite; rely on reproducible metrics. After training, run `python analyze/pq_experiment_statistics.py` or `python analyze/pq_experiment_attn_statistics.py` to refresh NDCG dashboards from `logs/pq_experiments`. When touching dataset tooling, rerun `make_dataset.py` for a sample language and inspect the resulting pickle via `analyze/look_pickle.py`. Attach averaged NDCG@10 figures or CSV diffs to code reviews.

## Commit & Pull Request Guidelines
Recent history uses short, present-tense commits (`add sq`, `ready for exp`); keep that voice while naming the affected area, e.g. `tune pq_head lr grid`. PRs should summarise the motivation, list reproduction commands (dataset prep, training, analysis), and include log snippets or CSV excerpts showing metric changes. Store large checkpoints externally and document their download path.

## Security & Configuration Tips
Remote adapters read credentials from environment variables such as `OPENAI_API_KEY` and `ANTHROPIC_API_KEY`; never commit `.env` files. Test new providers with small batches to avoid quota surprises, document required limits in scripts, and trim sensitive file paths from logs before sharing.

Always respond in Chinese-simlified.